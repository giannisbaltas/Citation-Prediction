{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc33eee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import csv\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "721be9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 138499\n",
      "Number of edges: 1091955\n"
     ]
    }
   ],
   "source": [
    "# Create a graph\n",
    "G = nx.read_edgelist('edgelist.txt', delimiter=',', create_using=nx.DiGraph(), nodetype=int)\n",
    "nodes = list(G.nodes())\n",
    "n = G.number_of_nodes()\n",
    "m = G.number_of_edges()\n",
    "print('Number of nodes:', n)\n",
    "print('Number of edges:', m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53fd7d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the abstract of each paper\n",
    "abstracts = dict()\n",
    "with open('abstracts.txt', 'r',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        node, abstract = line.split('|--|')\n",
    "        abstracts[int(node)] = abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b52d59fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map text to set of terms\n",
    "for node in abstracts:\n",
    "    abstracts[node] = set(abstracts[node].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "115644e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the authors of each paper\n",
    "authors = dict()\n",
    "with open('authors.txt', 'r',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        node, author = line.split('|--|')\n",
    "        authors[int(node)] = author "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc53424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map text to set of terms\n",
    "for node in authors:\n",
    "    authors[node] = set(authors[node].split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f89e7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# its class label is 1 if it corresponds to an edge and 0, otherwise.\n",
    "# Use the following 3 features for each pair of nodes:\n",
    "# (1) sum of number of unique terms of the two nodes' abstracts\n",
    "# (2) absolute value of difference of number of unique terms of the two nodes' abstracts\n",
    "# (3) number of common terms between the abstracts of the two nodes\n",
    "# (4) number of common authors\n",
    "# (5) The node degree is the number of edges adjacent to the node. \n",
    "#     The weighted node degree is the sum of the edge weights for edges incident to that node.\n",
    "# (6) absolute value of difference of the weighted node degree \n",
    "# (7) The node in_degree is the number of edges pointing to the node. \n",
    "# (8) absolute value of difference of the number od edges pointing to the node\n",
    "# (9) sum of degree centrality of the two nodes\n",
    "\n",
    "X_train = np.zeros((2*m, 9))\n",
    "y_train = np.zeros(2*m)\n",
    "n = G.number_of_nodes()\n",
    "DEGREE_CENTRALITY = nx.degree_centrality(G)\n",
    "# CLOSENESS_CENTRALITY = nx.closeness_centrality(G)\n",
    "# BETWEENNESS_CENTRALITY = networkx.betweenness_centrality(G)\n",
    "for i,edge in enumerate(G.edges()):\n",
    "    # an edge\n",
    "    X_train[i,0] = len(abstracts[edge[0]]) + len(abstracts[edge[1]])\n",
    "    X_train[i,1] = abs(len(abstracts[edge[0]]) - len(abstracts[edge[1]]))\n",
    "    X_train[i,2] = len(abstracts[edge[0]].intersection(abstracts[edge[1]]))\n",
    "    \n",
    "    X_train[i,3] = len(authors[edge[0]].intersection(authors[edge[1]]))\n",
    "    \n",
    "    X_train[i,4] = G.degree(edge[0]) + G.degree(edge[1]) \n",
    "    X_train[i,5] = abs(G.degree(edge[0]) - G.degree(edge[1]))\n",
    "    \n",
    "    X_train[i,6] = G.in_degree(edge[0]) + G.in_degree(edge[1])\n",
    "    X_train[i,7] = abs(G.in_degree(edge[0]) - G.in_degree(edge[1]))\n",
    "    \n",
    "    X_train[i,8] = DEGREE_CENTRALITY[edge[0]] + DEGREE_CENTRALITY[edge[1]]\n",
    "    \n",
    "    y_train[i] = 1\n",
    "\n",
    "    # a randomly generated pair of nodes\n",
    "    n1 = randint(0, n-1)\n",
    "    n2 = randint(0, n-1)\n",
    "    \n",
    "    #an edge\n",
    "    X_train[m+i,0] = len(abstracts[n1]) + len(abstracts[n2])\n",
    "    X_train[m+i,1] = abs(len(abstracts[n1]) - len(abstracts[n2]))\n",
    "    X_train[m+i,2] = len(abstracts[n1].intersection(abstracts[n2]))\n",
    "    \n",
    "    X_train[m+i,3] = len(authors[n1].intersection(authors[n2]))\n",
    "    \n",
    "    X_train[m+i,4] = G.degree(n1) + G.degree(n2) \n",
    "    X_train[m+i,5] = abs(G.degree(n1) - G.degree(n2))\n",
    "    \n",
    "    X_train[m+i,6] = G.in_degree(n1) + G.in_degree(n2)\n",
    "    X_train[m+i,7] = abs(G.in_degree(n1) - G.in_degree(n2))\n",
    "    \n",
    "    X_train[m+i,8] = DEGREE_CENTRALITY[n1] + DEGREE_CENTRALITY[n2]\n",
    "    \n",
    "    y_train[m+i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ed00d2f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training matrix: (2183910, 9)\n"
     ]
    }
   ],
   "source": [
    "print('Size of training matrix:', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d122061b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test data. Each sample is a pair of nodes\n",
    "node_pairs = list()\n",
    "with open('test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        node_pairs.append((int(t[0]), int(t[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d4cf836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of test matrix: (106692, 9)\n"
     ]
    }
   ],
   "source": [
    "# Create the test matrix. Use the same 8 features as above\n",
    "X_test = np.zeros((len(node_pairs), 9))\n",
    "for i,node_pair in enumerate(node_pairs):\n",
    "    X_test[i,0] = len(abstracts[node_pair[0]]) + len(abstracts[node_pair[1]])\n",
    "    X_test[i,1] = abs(len(abstracts[node_pair[0]]) - len(abstracts[node_pair[1]]))\n",
    "    X_test[i,2] = len(abstracts[node_pair[0]].intersection(abstracts[node_pair[1]]))\n",
    "    X_test[i,3] = len(authors[node_pair[0]].intersection(authors[node_pair[1]]))\n",
    "    X_test[i,4] = G.degree(node_pair[0]) + G.degree(node_pair[1])\n",
    "    X_test[i,5] = abs(G.degree(node_pair[0]) - G.degree(node_pair[1]))\n",
    "    X_test[i,6] = G.in_degree(node_pair[0]) + G.in_degree(node_pair[1])\n",
    "    X_test[i,7] = abs(G.in_degree(node_pair[0]) - G.in_degree(node_pair[1]))\n",
    "    X_test[i,8] = DEGREE_CENTRALITY[node_pair[0]] + DEGREE_CENTRALITY[node_pair[1]]\n",
    "\n",
    "print('Size of test matrix:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3b5c9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22da0a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use logistic regression to predict if two nodes are linked by an edge\n",
    "clf = LogisticRegression(solver='liblinear',random_state=34)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "y_pred = y_pred[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a82ae4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johnny\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Use logistic regression to predict if two nodes are linked by an edge\n",
    "# clf = LogisticRegression(solver='saga', C=1, n_jobs=-1,random_state=34)\n",
    "# clf.fit(X_train, y_train)\n",
    "# y_pred = clf.predict_proba(X_test)\n",
    "# y_pred = y_pred[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17585835",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  from sklearn.ensemble import RandomForestClassifier\n",
    "# clf = RandomForestClassifier(max_depth=3, random_state=42)\n",
    "# clf.fit(X_train, y_train)\n",
    "# y_pred = clf.predict_proba(X_test)\n",
    "# y_pred = y_pred[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c723814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predictions to a file\n",
    "predictions = zip(range(len(y_pred)), y_pred)\n",
    "with open(\"submission_text_graph_link_prediction.csv\",\"w\") as pred:\n",
    "    csv_out = csv.writer(pred)\n",
    "    csv_out.writerow(['id','predicted'])\n",
    "    for row in predictions:\n",
    "        csv_out.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
