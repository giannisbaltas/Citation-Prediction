{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b205d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import csv\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "# import gensim.downloader as api gia word embeddings\n",
    "# model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44039465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 138499\n",
      "Number of edges: 1091955\n"
     ]
    }
   ],
   "source": [
    "# Create a graph\n",
    "G = nx.read_edgelist('edgelist.txt', delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "nodes = list(G.nodes())\n",
    "n = G.number_of_nodes()\n",
    "m = G.number_of_edges()\n",
    "print('Number of nodes:', n)\n",
    "print('Number of edges:', m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b37e6b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the abstract of each paper\n",
    "abstracts = dict()\n",
    "with open('abstracts.txt', 'r',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        node, abstract = line.split('|--|')\n",
    "        abstracts[int(node)] = abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8abcd67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map text to set of terms\n",
    "for node in abstracts:\n",
    "    abstracts[node] = set(abstracts[node].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d99e0749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the authors of each paper\n",
    "authors = dict()\n",
    "with open('authors.txt', 'r',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        node, author = line.split('|--|')\n",
    "        authors[int(node)] = author    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "03e9e0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map text to set of terms\n",
    "for node in authors:\n",
    "    authors[node] = set(authors[node].split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9d5074af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# its class label is 1 if it corresponds to an edge and 0, otherwise.\n",
    "# Use the following 3 features for each pair of nodes:\n",
    "# (1) sum of number of unique terms of the two nodes' abstracts\n",
    "# (2) absolute value of difference of number of unique terms of the two nodes' abstracts\n",
    "# (3) number of common terms between the abstracts of the two nodes\n",
    "# (4) number of common authors of each paper\n",
    "X_train = np.zeros((2*m, 4))\n",
    "y_train = np.zeros(2*m)\n",
    "n = G.number_of_nodes()\n",
    "for i,edge in enumerate(G.edges()):\n",
    "    # an edge\n",
    "    X_train[i,0] = len(abstracts[edge[0]]) + len(abstracts[edge[1]])\n",
    "    X_train[i,1] = abs(len(abstracts[edge[0]]) - len(abstracts[edge[1]]))\n",
    "    X_train[i,2] = len(abstracts[edge[0]].intersection(abstracts[edge[1]]))\n",
    "    X_train[i,3] = len(authors[edge[0]].intersection(authors[edge[1]]))\n",
    "    y_train[i] = 1\n",
    "\n",
    "    # a randomly generated pair of nodes\n",
    "    n1 = randint(0, n-1)\n",
    "    n2 = randint(0, n-1)\n",
    "    \n",
    "    \n",
    "    X_train[m+i,0] = len(abstracts[n1]) + len(abstracts[n2])\n",
    "    X_train[m+i,1] = abs(len(abstracts[n1]) - len(abstracts[n2]))\n",
    "    X_train[m+i,2] = len(abstracts[n1].intersection(abstracts[n2]))\n",
    "    X_train[i,3] = len(authors[n1].intersection(authors[n2]))\n",
    "    y_train[m+i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0350f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.zeros((2*m, 4))\n",
    "y_train = np.zeros(2*m)\n",
    "n = G.number_of_nodes()\n",
    "for i,edge in enumerate(G.edges()):\n",
    "    # an edge\n",
    "    X_train[i,0] = G.degree(edge[0]) + G.degree(edge[0]) \n",
    "    X_train[i,1] = abs(G.degree(edge[1]) - G.degree(edge[1]) )\n",
    "    y_train[i] = 1\n",
    "\n",
    "    # a randomly generated pair of nodes\n",
    "    n1 = randint(0, n-1)\n",
    "    n2 = randint(0, n-1)\n",
    "    # an edge\n",
    "    X_train[m+i,0] = G.degree(edge[n1]) + G.degree(edge[n2]) \n",
    "    X_train[m+i,1] = abs(G.degree(edge[n1]) - G.degree(edge[n2]) )\n",
    "    y_train[m+i] = 0\n",
    "    \n",
    "    # και συνδυασμος τεξτ και γκραφ σε 0,1,2,3,4 \n",
    "    # το ιδιο και για train και για test\n",
    "    # text preprocessing\n",
    "    # lemmatizer(text) preprocess_str(sentence)[return ''.join(set(lemm_text))] preprosecc preprosecc_l stop_words(and the if etc.) for english\n",
    "    # kanoume return to set\n",
    "    # counter = {k: v for k,v in sorted()}\n",
    "#     f = open(\"counter.txt,\"r\")\n",
    "#              for ind,w in enumerate()\n",
    "#              f.write()\n",
    "#              if ind = 1000\n",
    "#                  most_c.append(w)\n",
    "#              f.close()\n",
    "    #\n",
    "    #    author1 = authors(edges[0]) kai gia 1 \n",
    "#     common_author = len(auth1.intersection(author2))\n",
    "#     Xtrain[i,3] = common_author\n",
    "#     allo feauture kai me G.in_degree([1])\n",
    "#     # word embeddings\n",
    "#     sen1 = abstracts[n1]\n",
    "#     sen2 = ...\n",
    "#     sim = cos_sim(embeddings[0].detach().numpy(), kai gia to 1)\n",
    "#     sim = cos_sim(X[edge[0],:], X[edge[1],:])\n",
    "#     sim = model.wmdinstance(sen1,sen2)\n",
    "#     X_train[i,5] = sim\n",
    "    \n",
    "    # fast text ths fb\n",
    "    \n",
    "    # networkx centrality degree eigenvector closeness klp\n",
    "    # networkx link prediction etoimoi algorithmoi gia extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f6272776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training matrix: (2183910, 4)\n"
     ]
    }
   ],
   "source": [
    "print('Size of training matrix:', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6b9db476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test data. Each sample is a pair of nodes\n",
    "node_pairs = list()\n",
    "with open('test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        node_pairs.append((int(t[0]), int(t[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4b07b11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of test matrix: (106692, 4)\n"
     ]
    }
   ],
   "source": [
    "# Create the test matrix. Use the same 4 features as above\n",
    "X_test = np.zeros((len(node_pairs), 4))\n",
    "for i,node_pair in enumerate(node_pairs):\n",
    "    X_test[i,0] = len(abstracts[node_pair[0]]) + len(abstracts[node_pair[1]])\n",
    "    X_test[i,1] = abs(len(abstracts[node_pair[0]]) - len(abstracts[node_pair[1]]))\n",
    "    X_test[i,2] = len(abstracts[node_pair[0]].intersection(abstracts[node_pair[1]]))\n",
    "    X_test[i,3] = len(authors[node_pair[0]].intersection(authors[node_pair[1]]))\n",
    "\n",
    "print('Size of test matrix:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0f5127a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "274c5ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use logistic regression to predict if two nodes are linked by an edge\n",
    "clf = LogisticRegression(solver='liblinear',random_state=34)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "y_pred = y_pred[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fc858123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predictions to a file\n",
    "predictions = zip(range(len(y_pred)), y_pred)\n",
    "with open(\"submission_text_baseline1.csv\",\"w\") as pred:\n",
    "    csv_out = csv.writer(pred)\n",
    "    csv_out.writerow(['id','predicted'])\n",
    "    for row in predictions:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0776b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
